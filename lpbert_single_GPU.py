import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import time
import os
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from collections import Counter
import math
import warnings
warnings.filterwarnings('ignore')

# è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

# =====================================================
# GPUä¼˜åŒ–çš„æ¨¡å‹å®ç°
# =====================================================

class OptimizedLPBERTEmbedding(nn.Module):
    """GPUä¼˜åŒ–çš„LP-BERTåµŒå…¥å±‚"""
    
    def __init__(self, config):
        super().__init__()
        
        self.embed_size = config['embed_size']
        
        # åµŒå…¥å±‚
        self.day_embedding = nn.Embedding(config['max_days'], self.embed_size)
        self.time_embedding = nn.Embedding(config['max_times'], self.embed_size)
        self.location_embedding = nn.Embedding(config['max_locations'], self.embed_size)
        self.timedelta_embedding = nn.Embedding(config['max_timedelta'], self.embed_size)
        
        # å±‚æ ‡å‡†åŒ–å’Œdropout
        self.layer_norm = nn.LayerNorm(self.embed_size, eps=1e-6)
        self.dropout = nn.Dropout(config['dropout'])
        
        # ä¼˜åŒ–æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """ä¼˜åŒ–çš„æƒé‡åˆå§‹åŒ–"""
        for module in [self.day_embedding, self.time_embedding, 
                      self.location_embedding, self.timedelta_embedding]:
            nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def forward(self, day_ids, time_ids, location_ids, timedelta_ids):
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰åµŒå…¥
        day_emb = self.day_embedding(day_ids)
        time_emb = self.time_embedding(time_ids)
        location_emb = self.location_embedding(location_ids)
        timedelta_emb = self.timedelta_embedding(timedelta_ids)
        
        # åœ¨GPUä¸Šé«˜æ•ˆæ±‚å’Œ
        embeddings = day_emb + time_emb + location_emb + timedelta_emb
        
        # åº”ç”¨å±‚æ ‡å‡†åŒ–å’Œdropout
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        
        return embeddings

class OptimizedLPBERTModel(nn.Module):
    """GPUä¼˜åŒ–çš„LP-BERTä¸»æ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        self.embed_size = config['embed_size']
        self.num_layers = config['num_layers']
        self.num_heads = config['num_heads']
        self.max_locations = config['max_locations']
        
        # åµŒå…¥å±‚
        self.embedding = OptimizedLPBERTEmbedding(config)
        
        # ä¼˜åŒ–çš„Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embed_size,
            nhead=self.num_heads,
            dim_feedforward=config['hidden_size'],
            dropout=config['dropout'],
            activation='gelu',  # GELUé€šå¸¸æ¯”ReLUæ›´å¥½
            batch_first=True,
            norm_first=True  # Pre-LNå¯èƒ½æ›´ç¨³å®š
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=self.num_layers,
            enable_nested_tensor=False  # A100ä¸Šå¯èƒ½æ›´å¿«
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(self.embed_size, self.max_locations)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """ä¼˜åŒ–çš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, day_ids, time_ids, location_ids, timedelta_ids, attention_mask=None):
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""
        # åµŒå…¥
        embeddings = self.embedding(day_ids, time_ids, location_ids, timedelta_ids)
        
        # åˆ›å»ºé«˜æ•ˆçš„attention mask
        if attention_mask is None:
            # é¿å…åˆ›å»ºä¸å¿…è¦çš„mask
            encoded = self.transformer(embeddings)
        else:
            # è½¬æ¢ä¸ºTransformeréœ€è¦çš„maskæ ¼å¼
            src_key_padding_mask = (attention_mask == 0)
            encoded = self.transformer(embeddings, src_key_padding_mask=src_key_padding_mask)
        
        # è¾“å‡ºé¢„æµ‹
        logits = self.output_layer(encoded)
        
        return logits

# =====================================================
# è®ºæ–‡è¯„ä¼°æŒ‡æ ‡å®ç°ï¼šGEO-BLEUå’ŒDTW
# =====================================================

class GEOBLEUCalculator:
    """GEO-BLEUè¯„ä¼°æŒ‡æ ‡ - æŒ‰è®ºæ–‡å®ç°"""
    
    def __init__(self, max_n=4):
        self.max_n = max_n
    
    def _get_ngrams(self, trajectory: List[Tuple[int, int]], n: int) -> List[Tuple]:
        """è·å–è½¨è¿¹çš„n-gram"""
        if len(trajectory) < n:
            return []
        
        ngrams = []
        for i in range(len(trajectory) - n + 1):
            ngram = tuple(trajectory[i:i+n])
            ngrams.append(ngram)
        
        return ngrams
    
    def _compute_bleu_score(self, pred_trajectory: List[Tuple[int, int]], 
                           true_trajectory: List[Tuple[int, int]]) -> float:
        """è®¡ç®—å•ä¸ªè½¨è¿¹çš„BLEUåˆ†æ•°"""
        if len(pred_trajectory) == 0 or len(true_trajectory) == 0:
            return 0.0
        
        # è®¡ç®—å„é˜¶n-gramçš„ç²¾ç¡®åº¦
        precisions = []
        
        for n in range(1, self.max_n + 1):
            pred_ngrams = self._get_ngrams(pred_trajectory, n)
            true_ngrams = self._get_ngrams(true_trajectory, n)
            
            if len(pred_ngrams) == 0:
                precisions.append(0.0)
                continue
            
            # ç»Ÿè®¡n-gramå‡ºç°æ¬¡æ•°
            pred_counter = Counter(pred_ngrams)
            true_counter = Counter(true_ngrams)
            
            # è®¡ç®—åŒ¹é…çš„n-gramæ•°é‡
            matches = 0
            for ngram, count in pred_counter.items():
                matches += min(count, true_counter.get(ngram, 0))
            
            # è®¡ç®—ç²¾ç¡®åº¦
            precision = matches / len(pred_ngrams) if len(pred_ngrams) > 0 else 0.0
            precisions.append(precision)
        
        # è®¡ç®—å‡ ä½•å¹³å‡
        if all(p > 0 for p in precisions):
            geo_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))
        else:
            geo_mean = 0.0
        
        # é•¿åº¦æƒ©ç½š
        bp = min(1.0, len(pred_trajectory) / len(true_trajectory)) if len(true_trajectory) > 0 else 0.0
        
        return bp * geo_mean
    
    def compute_geobleu(self, predictions: Dict[int, Dict[int, List[Tuple[int, int]]]], 
                       ground_truth: Dict[int, Dict[int, List[Tuple[int, int]]]]) -> float:
        """è®¡ç®—GEO-BLEUåˆ†æ•°ï¼ˆæŒ‰å¤©è®¡ç®—ï¼‰"""
        total_score = 0.0
        total_count = 0
        
        for user_id in predictions:
            if user_id not in ground_truth:
                continue
            
            for day in predictions[user_id]:
                if day not in ground_truth[user_id]:
                    continue
                
                pred_traj = predictions[user_id][day]
                true_traj = ground_truth[user_id][day]
                
                if len(pred_traj) > 0 and len(true_traj) > 0:
                    score = self._compute_bleu_score(pred_traj, true_traj)
                    total_score += score
                    total_count += 1
        
        return total_score / total_count if total_count > 0 else 0.0

class DTWCalculator:
    """Dynamic Time Warping (DTW) è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        pass
    
    def _euclidean_distance(self, p1: Tuple[int, int], p2: Tuple[int, int]) -> float:
        """è®¡ç®—ä¸¤ç‚¹é—´çš„æ¬§æ°è·ç¦»"""
        return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
    
    def compute_dtw(self, pred_trajectory: List[Tuple[int, int]], 
                   true_trajectory: List[Tuple[int, int]]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè½¨è¿¹ä¹‹é—´çš„DTWè·ç¦»"""
        if len(pred_trajectory) == 0 or len(true_trajectory) == 0:
            return float('inf')
        
        n, m = len(pred_trajectory), len(true_trajectory)
        
        # åˆ›å»ºDTWçŸ©é˜µ
        dtw_matrix = [[float('inf')] * m for _ in range(n)]
        
        # åˆå§‹åŒ–
        dtw_matrix[0][0] = self._euclidean_distance(pred_trajectory[0], true_trajectory[0])
        
        # å¡«å……ç¬¬ä¸€è¡Œ
        for j in range(1, m):
            dtw_matrix[0][j] = dtw_matrix[0][j-1] + self._euclidean_distance(
                pred_trajectory[0], true_trajectory[j])
        
        # å¡«å……ç¬¬ä¸€åˆ—
        for i in range(1, n):
            dtw_matrix[i][0] = dtw_matrix[i-1][0] + self._euclidean_distance(
                pred_trajectory[i], true_trajectory[0])
        
        # å¡«å……å…¶ä½™å…ƒç´ 
        for i in range(1, n):
            for j in range(1, m):
                cost = self._euclidean_distance(pred_trajectory[i], true_trajectory[j])
                dtw_matrix[i][j] = cost + min(
                    dtw_matrix[i-1][j],      # æ’å…¥
                    dtw_matrix[i][j-1],      # åˆ é™¤
                    dtw_matrix[i-1][j-1]     # åŒ¹é…
                )
        
        return dtw_matrix[n-1][m-1]
    
    def compute_average_dtw(self, predictions: Dict[int, List[Tuple[int, int]]], 
                           ground_truth: Dict[int, List[Tuple[int, int]]]) -> float:
        """è®¡ç®—å¹³å‡DTWè·ç¦»"""
        total_dtw = 0.0
        total_count = 0
        
        for user_id in predictions:
            if user_id not in ground_truth:
                continue
            
            pred_traj = predictions[user_id]
            true_traj = ground_truth[user_id]
            
            if len(pred_traj) > 0 and len(true_traj) > 0:
                dtw_dist = self.compute_dtw(pred_traj, true_traj)
                if dtw_dist != float('inf'):
                    total_dtw += dtw_dist
                    total_count += 1
        
        return total_dtw / total_count if total_count > 0 else float('inf')

# =====================================================
# é«˜æ•ˆæ•°æ®é›†å®ç°
# =====================================================

def optimized_collate_fn(batch):
    """GPUä¼˜åŒ–çš„collateå‡½æ•° - ä¿®å¤å¤šè¿›ç¨‹CUDAé—®é¢˜"""
    # è·å–æ‰¹æ¬¡ä¸­çš„æœ€å¤§åºåˆ—é•¿åº¦
    max_length = max([item['actual_length'] for item in batch])
    batch_size = len(batch)
    
    # ç›´æ¥åœ¨CPUä¸Šåˆ›å»ºå¼ é‡ï¼Œé¿å…CUDAå¤šè¿›ç¨‹é—®é¢˜
    days = torch.zeros((batch_size, max_length), dtype=torch.long)
    times = torch.zeros((batch_size, max_length), dtype=torch.long)
    locations = torch.zeros((batch_size, max_length), dtype=torch.long)
    timedeltas = torch.zeros((batch_size, max_length), dtype=torch.long)
    attention_mask = torch.zeros((batch_size, max_length), dtype=torch.float32)
    labels = torch.zeros((batch_size, max_length), dtype=torch.long)
    
    mask_positions = []
    
    # æ‰¹é‡å¡«å……æ•°æ®
    for i, item in enumerate(batch):
        seq_len = item['actual_length']
        
        days[i, :seq_len] = item['days'][:seq_len]
        times[i, :seq_len] = item['times'][:seq_len]
        locations[i, :seq_len] = item['locations'][:seq_len]
        timedeltas[i, :seq_len] = item['timedeltas'][:seq_len]
        attention_mask[i, :seq_len] = item['attention_mask'][:seq_len]
        labels[i, :seq_len] = item['labels'][:seq_len]
        
        mask_positions.append(item['mask_positions'])
    
    return {
        'days': days,
        'times': times,
        'locations': locations,
        'timedeltas': timedeltas,
        'attention_mask': attention_mask,
        'labels': labels,
        'mask_positions': mask_positions
    }

class OptimizedHuMobTrainingDataset(Dataset):
    """GPUä¼˜åŒ–çš„è®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_file, seq_length=512, location_vocab=None, force_regenerate_sequences=False):
        self.seq_length = seq_length
        self.data_file = data_file
        
        print(f"åŠ è½½GPUä¼˜åŒ–è®­ç»ƒæ•°æ®é›†: {data_file}")
        
        # ç¼“å­˜è®¾ç½®
        cache_dir = "cache"
        os.makedirs(cache_dir, exist_ok=True)
        
        self.sequences_cache_file = os.path.join(cache_dir, f"optimized_sequences_{seq_length}.pkl")
        self.vocab_cache_file = os.path.join(cache_dir, "location_vocab.pkl")
        
        # æ£€æŸ¥ç¼“å­˜
        if self._can_use_cache() and not force_regenerate_sequences:
            print("ğŸš€ ä½¿ç”¨ç¼“å­˜å¿«é€ŸåŠ è½½...")
            self._load_from_cache()
            print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆï¼")
            print(f"   è®­ç»ƒåºåˆ—æ•°é‡: {len(self.sequences)}")
            print(f"   ä½ç½®è¯æ±‡è¡¨å¤§å°: {len(self.location_vocab)}")
            return
        
        # å®Œæ•´å¤„ç†
        print("ğŸ“Š å¼€å§‹GPUä¼˜åŒ–æ•°æ®å¤„ç†...")
        self._full_processing(location_vocab)
        self._save_to_cache()
        
        print(f"âœ… GPUä¼˜åŒ–æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"   è®­ç»ƒåºåˆ—æ•°é‡: {len(self.sequences)}")
        print(f"   ä½ç½®è¯æ±‡è¡¨å¤§å°: {len(self.location_vocab)}")
    
    def _can_use_cache(self):
        """æ£€æŸ¥ç¼“å­˜å¯ç”¨æ€§"""
        return (os.path.exists(self.sequences_cache_file) and 
                os.path.exists(self.vocab_cache_file))
    
    def _load_from_cache(self):
        """ä»ç¼“å­˜åŠ è½½"""
        with open(self.sequences_cache_file, 'rb') as f:
            cache_data = pickle.load(f)
            self.sequences = cache_data['sequences']
            
        with open(self.vocab_cache_file, 'rb') as f:
            self.location_vocab = pickle.load(f)
    
    def _save_to_cache(self):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_data = {
            'sequences': self.sequences,
            'seq_length': self.seq_length,
            'timestamp': time.time()
        }
        
        with open(self.sequences_cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        
        with open(self.vocab_cache_file, 'wb') as f:
            pickle.dump(self.location_vocab, f)
    
    def _full_processing(self, location_vocab):
        """å®Œæ•´æ•°æ®å¤„ç†"""
        # ä½¿ç”¨pandasçš„å‘é‡åŒ–æ“ä½œåŠ é€Ÿ
        self.data = pd.read_csv(self.data_file)
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {self.data.shape}")
        
        # é«˜æ•ˆè¿‡æ»¤
        complete_data = self.data[
            (self.data['uid'] < 80000) & 
            (self.data['x'] != 999) & 
            (self.data['y'] != 999)
        ].copy()
        
        self.data = complete_data
        print(f"å®Œæ•´æ•°æ®å½¢çŠ¶: {self.data.shape}")
        
        # æ„å»ºè¯æ±‡è¡¨
        if location_vocab is None:
            print("ğŸ—º æ„å»ºä½ç½®è¯æ±‡è¡¨...")
            self.location_vocab = self._build_location_vocab()
        else:
            self.location_vocab = location_vocab
        
        # é¢„å¤„ç†åºåˆ—
        print("ğŸ”„ é¢„å¤„ç†ç”¨æˆ·åºåˆ—ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰...")
        self.sequences = self._prepare_sequences_optimized()
    
    def _build_location_vocab(self):
        """ä¼˜åŒ–çš„è¯æ±‡è¡¨æ„å»º"""
        # ä½¿ç”¨pandasçš„é«˜æ•ˆå»é‡
        unique_locations = self.data[['x', 'y']].drop_duplicates()
        print(f"   å”¯ä¸€ä½ç½®æ•°é‡: {len(unique_locations)}")
        
        location_vocab = {
            '<PAD>': 0,
            '<MASK>': 1, 
            '<UNK>': 2
        }
        
        # å‘é‡åŒ–æ„å»ºè¯æ±‡è¡¨
        for idx, (x, y) in enumerate(unique_locations.values):
            location_key = (int(x), int(y))
            location_vocab[location_key] = idx + 3
        
        return location_vocab
    
    def _get_location_id(self, x, y):
        """è·å–ä½ç½®ID"""
        location_key = (int(x), int(y))
        return self.location_vocab.get(location_key, self.location_vocab['<UNK>'])
    
    def _prepare_sequences_optimized(self):
        """GPUä¼˜åŒ–çš„åºåˆ—å‡†å¤‡"""
        sequences = []
        unique_users = self.data['uid'].unique()
        
        print(f"   å¤„ç† {len(unique_users)} ä¸ªç”¨æˆ·...")
        
        # ä½¿ç”¨groupbyä¼˜åŒ–å¤„ç†
        user_groups = self.data.groupby('uid')
        
        for uid in tqdm(unique_users, desc="å¤„ç†ç”¨æˆ·åºåˆ—"):
            user_data = user_groups.get_group(uid).sort_values(['d', 't'])
            
            if len(user_data) < 40:  # æé«˜æœ€å°åºåˆ—è¦æ±‚
                continue
            
            # å‘é‡åŒ–è®¡ç®—æ—¶é—´å·®
            user_data = user_data.copy()
            user_data['prev_time'] = user_data['d'] * 48 + user_data['t']
            user_data['timedelta'] = user_data['prev_time'].diff().fillna(1).astype(int)
            user_data['timedelta'] = user_data['timedelta'].clip(0, 100)
            
            # åˆ›å»ºæ›´å¤šé‡å åºåˆ—ä»¥å¢åŠ è®­ç»ƒæ•°æ®
            step_size = self.seq_length // 8  # æ›´å°çš„æ­¥é•¿ï¼Œæ›´å¤šåºåˆ—
            
            for i in range(0, len(user_data) - self.seq_length + 1, step_size):
                seq_data = user_data.iloc[i:i + self.seq_length]
                
                if len(seq_data) != self.seq_length:
                    continue
                
                # å‘é‡åŒ–ç‰¹å¾æå–
                days = seq_data['d'].values
                times = seq_data['t'].values
                locations = np.array([self._get_location_id(x, y) 
                                    for x, y in zip(seq_data['x'], seq_data['y'])])
                timedeltas = seq_data['timedelta'].values
                
                sequences.append({
                    'uid': uid,
                    'days': days,
                    'times': times,
                    'locations': locations,
                    'timedeltas': timedeltas,
                    'length': self.seq_length
                })
        
        print(f"   ç”Ÿæˆ {len(sequences)} ä¸ªè®­ç»ƒåºåˆ—")
        return sequences
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        seq = self.sequences[idx]
        seq_len = self.seq_length
        
        # é«˜æ•ˆæ•°ç»„æ“ä½œ
        days = np.array(seq['days'], dtype=np.long)
        times = np.array(seq['times'], dtype=np.long)
        locations = np.array(seq['locations'], dtype=np.long)
        timedeltas = np.array(seq['timedeltas'], dtype=np.long)
        
        # åˆ›å»ºæ ‡ç­¾
        labels = locations.copy()
        
        # ä¼˜åŒ–çš„æ©ç ç­–ç•¥
        input_locations = locations.copy()
        mask_positions = []
        
        alpha_days = 15
        unique_days = np.unique(days)
        
        if len(unique_days) >= alpha_days:
            start_day_idx = np.random.randint(0, len(unique_days) - alpha_days + 1)
            mask_start_day = unique_days[start_day_idx]
            mask_end_day = unique_days[min(start_day_idx + alpha_days - 1, len(unique_days) - 1)]
            
            # å‘é‡åŒ–æ©ç æ“ä½œ
            mask_condition = (days >= mask_start_day) & (days <= mask_end_day)
            mask_positions = np.where(mask_condition)[0].tolist()
            input_locations[mask_condition] = self.location_vocab['<MASK>']
        else:
            # éšæœºæ©ç 
            mask_prob = np.random.random(len(locations))
            mask_condition = mask_prob < 0.15
            mask_positions = np.where(mask_condition)[0].tolist()
            input_locations[mask_condition] = self.location_vocab['<MASK>']
        
        return {
            'days': torch.tensor(days, dtype=torch.long),
            'times': torch.tensor(times, dtype=torch.long),
            'locations': torch.tensor(input_locations, dtype=torch.long),
            'timedeltas': torch.tensor(timedeltas, dtype=torch.long),
            'attention_mask': torch.ones(seq_len, dtype=torch.float32),
            'labels': torch.tensor(labels, dtype=torch.long),
            'mask_positions': mask_positions,
            'actual_length': seq_len
        }

# =====================================================
# GPUä¼˜åŒ–çš„è®­ç»ƒå™¨ - åŒ…å«è®ºæ–‡è¯„ä¼°æŒ‡æ ‡
# =====================================================

class OptimizedLPBERTTrainer:
    """GPUä¼˜åŒ–çš„LP-BERTè®­ç»ƒå™¨ - åŒ…å«è®ºæ–‡è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self, model, config, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.config = config
        self.device = device
        
        # å¯ç”¨ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(torch, 'compile') and torch.cuda.is_available():
            print("ğŸš€ å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–...")
            try:
                self.model = torch.compile(self.model, mode='max-autotune')
            except Exception as e:
                print(f"âš ï¸ ç¼–è¯‘ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼: {e}")
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            eps=1e-8,
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - Cosineé€€ç«
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=config['num_epochs'],
            eta_min=config['learning_rate'] * 0.01
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if torch.cuda.is_available() else None
        self.use_amp = torch.cuda.is_available()
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)
        
        # è®ºæ–‡è¯„ä¼°æŒ‡æ ‡
        self.geobleu_calc = GEOBLEUCalculator()
        self.dtw_calc = DTWCalculator()
        
        # è®°å½•
        self.train_history = []
        
        print(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  æ··åˆç²¾åº¦: {self.use_amp}")
        print(f"  æ¨¡å‹ç¼–è¯‘: {hasattr(self.model, '_orig_mod')}")
        print(f"  è¯„ä¼°æŒ‡æ ‡: GEO-BLEU + DTW (è®ºæ–‡æ ‡å‡†)")
    
    def _convert_to_coordinates(self, location_ids, location_vocab):
        """å°†ä½ç½®IDè½¬æ¢ä¸ºåæ ‡"""
        id_to_location = {v: k for k, v in location_vocab.items() if isinstance(k, tuple)}
        coordinates = []
        for loc_id in location_ids:
            if loc_id in id_to_location:
                coordinates.append(id_to_location[loc_id])
            else:
                coordinates.append((100, 100))  # é»˜è®¤åæ ‡
        return coordinates
    
    def _evaluate_paper_metrics(self, train_loader, location_vocab, sample_size=100):
        """ä½¿ç”¨è®ºæ–‡æ ‡å‡†è¯„ä¼°æ¨¡å‹ï¼ˆé‡‡æ ·è¯„ä¼°ä»¥èŠ‚çœæ—¶é—´ï¼‰"""
        self.model.eval()
        
        predictions_dict = {}
        ground_truth_dict = {}
        
        sample_count = 0
        
        with torch.no_grad():
            for batch in train_loader:
                if sample_count >= sample_size:
                    break
                
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                days = batch['days'].to(self.device, non_blocking=True)
                times = batch['times'].to(self.device, non_blocking=True)
                locations = batch['locations'].to(self.device, non_blocking=True)
                timedeltas = batch['timedeltas'].to(self.device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # å‰å‘ä¼ æ’­
                if self.use_amp:
                    with autocast():
                        logits = self.model(days, times, locations, timedeltas, attention_mask)
                else:
                    logits = self.model(days, times, locations, timedeltas, attention_mask)
                
                predictions = torch.argmax(logits, dim=-1)
                
                # å¤„ç†æ¯ä¸ªæ ·æœ¬
                for i in range(min(len(batch['mask_positions']), sample_size - sample_count)):
                    if sample_count >= sample_size:
                        break
                    
                    mask_positions = batch['mask_positions'][i]
                    if len(mask_positions) == 0:
                        continue
                    
                    # æå–æ©ç ä½ç½®çš„é¢„æµ‹å’ŒçœŸå®å€¼
                    pred_ids = predictions[i, mask_positions].cpu().numpy()
                    true_ids = labels[i, mask_positions].cpu().numpy()
                    day_values = days[i, mask_positions].cpu().numpy()
                    
                    # è½¬æ¢ä¸ºåæ ‡
                    pred_coords = self._convert_to_coordinates(pred_ids, location_vocab)
                    true_coords = self._convert_to_coordinates(true_ids, location_vocab)
                    
                    # æŒ‰å¤©åˆ†ç»„
                    user_id = sample_count  # ä½¿ç”¨æ ·æœ¬è®¡æ•°ä½œä¸ºç”¨æˆ·ID
                    predictions_dict[user_id] = {}
                    ground_truth_dict[user_id] = {}
                    
                    for j, day in enumerate(day_values):
                        day = int(day)
                        if day not in predictions_dict[user_id]:
                            predictions_dict[user_id][day] = []
                            ground_truth_dict[user_id][day] = []
                        
                        predictions_dict[user_id][day].append(pred_coords[j])
                        ground_truth_dict[user_id][day].append(true_coords[j])
                    
                    sample_count += 1
        
        # è®¡ç®—è®ºæ–‡æŒ‡æ ‡
        geobleu_score = self.geobleu_calc.compute_geobleu(predictions_dict, ground_truth_dict)
        
        # ä¸ºDTWåˆ›å»ºå®Œæ•´è½¨è¿¹
        pred_full_trajectories = {}
        true_full_trajectories = {}
        
        for user_id in predictions_dict:
            pred_traj = []
            true_traj = []
            
            for day in sorted(predictions_dict[user_id].keys()):
                pred_traj.extend(predictions_dict[user_id][day])
                true_traj.extend(ground_truth_dict[user_id][day])
            
            pred_full_trajectories[user_id] = pred_traj
            true_full_trajectories[user_id] = true_traj
        
        dtw_score = self.dtw_calc.compute_average_dtw(pred_full_trajectories, true_full_trajectories)
        
        return geobleu_score, dtw_score
    
    def train_epoch(self, train_loader, epoch, location_vocab=None):
        """GPUä¼˜åŒ–çš„è®­ç»ƒepoch - æ·»åŠ è®ºæ–‡è¯„ä¼°"""
        self.model.train()
        total_loss = 0
        total_predictions = 0
        correct_predictions = 0
        
        # å¯ç”¨cudnn benchmark
        torch.backends.cudnn.benchmark = True
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} Training (GPUä¼˜åŒ–)')
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # éé˜»å¡æ•°æ®ä¼ è¾“
                days = batch['days'].to(self.device, non_blocking=True)
                times = batch['times'].to(self.device, non_blocking=True)
                locations = batch['locations'].to(self.device, non_blocking=True)
                timedeltas = batch['timedeltas'].to(self.device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
                labels = batch['labels'].to(self.device, non_blocking=True)
                
                # æ¸…é›¶æ¢¯åº¦
                self.optimizer.zero_grad(set_to_none=True)
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                if self.use_amp:
                    with autocast():
                        logits = self.model(days, times, locations, timedeltas, attention_mask)
                        loss = self._compute_masked_loss(logits, locations, labels, batch['mask_positions'])
                    
                    # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
                    logits = self.model(days, times, locations, timedeltas, attention_mask)
                    loss = self._compute_masked_loss(logits, locations, labels, batch['mask_positions'])
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                
                # è®¡ç®—ç®€å•å‡†ç¡®ç‡
                with torch.no_grad():
                    predictions = torch.argmax(logits, dim=-1)
                    for i, mask_pos_list in enumerate(batch['mask_positions']):
                        for pos in mask_pos_list:
                            if pos < predictions.size(1):
                                if predictions[i, pos] == labels[i, pos]:
                                    correct_predictions += 1
                                total_predictions += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                if batch_idx % 20 == 0:  
                    avg_loss = total_loss / (batch_idx + 1)
                    accuracy = correct_predictions / max(total_predictions, 1) * 100
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
                    
                    pbar.set_postfix({
                        'Loss': f'{avg_loss:.4f}',
                        'Acc': f'{accuracy:.1f}%',
                        'VRAM': f'{gpu_memory:.1f}GB'
                    })
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                raise e
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / max(total_predictions, 1) * 100
        
        # è®ºæ–‡è¯„ä¼°æŒ‡æ ‡ï¼ˆæ¯2ä¸ªepochè¯„ä¼°ä¸€æ¬¡ï¼‰
        geobleu_score = 0.0
        dtw_score = float('inf')
        
        if location_vocab is not None and (epoch + 1) % 2 == 0:
            print(f"\nğŸ” è¿›è¡Œè®ºæ–‡æ ‡å‡†è¯„ä¼°...")
            try:
                geobleu_score, dtw_score = self._evaluate_paper_metrics(train_loader, location_vocab)
                print(f"ğŸ“Š è®ºæ–‡æŒ‡æ ‡ - GEO-BLEU: {geobleu_score:.4f}, DTW: {dtw_score:.4f}")
            except Exception as e:
                print(f"âš ï¸ è®ºæ–‡è¯„ä¼°å¤±è´¥: {e}")
        
        return avg_loss, accuracy, geobleu_score, dtw_score
    
    def _compute_masked_loss(self, logits, input_locations, true_labels, mask_positions_batch):
        """GPUä¼˜åŒ–çš„æŸå¤±è®¡ç®—"""
        all_masked_logits = []
        all_masked_labels = []
        
        for i, mask_positions in enumerate(mask_positions_batch):
            if len(mask_positions) == 0:
                continue
            
            mask_tensor = torch.tensor(mask_positions, device=logits.device)
            if len(mask_tensor) > 0 and mask_tensor.max() < logits.size(1):
                masked_logits = logits[i, mask_tensor]
                masked_labels = true_labels[i, mask_tensor]
                
                all_masked_logits.append(masked_logits)
                all_masked_labels.append(masked_labels)
        
        if len(all_masked_logits) == 0:
            return torch.tensor(0.0, requires_grad=True, device=logits.device)
        
        combined_logits = torch.cat(all_masked_logits, dim=0)
        combined_labels = torch.cat(all_masked_labels, dim=0)
        
        loss = self.criterion(combined_logits, combined_labels)
        return loss
    
    def train(self, train_loader, num_epochs, location_vocab=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - åŒ…å«è®ºæ–‡è¯„ä¼°"""
        print(f"å¼€å§‹GPUä¼˜åŒ–LP-BERTè®­ç»ƒ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ··åˆç²¾åº¦: {self.use_amp}")
        print(f"è®­ç»ƒepochs: {num_epochs}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"è¯„ä¼°æ–¹å¼: ç®€å•å‡†ç¡®ç‡ + GEO-BLEU + DTW (è®ºæ–‡æ ‡å‡†)")
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        best_loss = float('inf')
        best_geobleu = 0.0
        patience_counter = 0
        max_patience = 3
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            print("-" * 60)
            
            epoch_start_time = time.time()
            
            try:
                # è®­ç»ƒ
                train_loss, train_acc, geobleu_score, dtw_score = self.train_epoch(
                    train_loader, epoch, location_vocab
                )
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                
                epoch_time = time.time() - epoch_start_time
                
                # è®°å½•å†å²
                self.train_history.append({
                    'epoch': epoch + 1,
                    'train_loss': train_loss,
                    'train_acc': train_acc,
                    'geobleu': geobleu_score,
                    'dtw': dtw_score,
                    'lr': current_lr,
                    'epoch_time': epoch_time
                })
                
                print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | ç®€å•å‡†ç¡®ç‡: {train_acc:.2f}%")
                print(f"å­¦ä¹ ç‡: {current_lr:.6f} | è€—æ—¶: {epoch_time:.1f}s")
                
                # æ˜¾ç¤ºè®ºæ–‡è¯„ä¼°ç»“æœ
                if geobleu_score > 0:
                    print(f"ğŸ“Š è®ºæ–‡è¯„ä¼° - GEO-BLEU: {geobleu_score:.4f} | DTW: {dtw_score:.4f}")
                    print(f"ğŸ¯ è®ºæ–‡å¯¹æ¯” - ç›®æ ‡GEO-BLEU: 0.3440 | ç›®æ ‡DTW: 29.9633")
                    
                    progress = geobleu_score / 0.3440 * 100
                    print(f"ğŸ“ˆ GEO-BLEUè¿›åº¦: {progress:.1f}% of è®ºæ–‡ç»“æœ")
                
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.max_memory_allocated() / 1024**3
                    print(f"GPUå³°å€¼å†…å­˜: {gpu_memory:.1f}GB")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                model_improved = False
                if geobleu_score > 0 and geobleu_score > best_geobleu:
                    best_geobleu = geobleu_score
                    model_improved = True
                    save_reason = f"GEO-BLEU: {geobleu_score:.4f}"
                elif geobleu_score == 0 and train_loss < best_loss:
                    best_loss = train_loss
                    model_improved = True
                    save_reason = f"æŸå¤±: {train_loss:.4f}"
                
                if model_improved:
                    patience_counter = 0
                    self.save_model(f'optimized_lpbert_best_epoch_{epoch+1}.pth')
                    print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ ({save_reason})")
                else:
                    patience_counter += 1
                
                # æ—©åœ
                if patience_counter >= max_patience:
                    print(f"æ—©åœï¼šæ¨¡å‹è¿ç»­{max_patience}ä¸ªepochæœªæ”¹å–„")
                    break
                    
            except Exception as e:
                print(f"âŒ è®­ç»ƒepoch {epoch+1}å¤±è´¥: {e}")
                self.save_model(f'optimized_lpbert_interrupted_epoch_{epoch+1}.pth')
                raise e
        
        print(f"\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"æœ€ä½³GEO-BLEU: {best_geobleu:.4f}")
        
        # æœ€ç»ˆè¯„ä¼°æ€»ç»“
        if len(self.train_history) > 0:
            final_stats = self.train_history[-1]
            print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒç»“æœ:")
            print(f"  ç®€å•å‡†ç¡®ç‡: {final_stats['train_acc']:.2f}%")
            if final_stats['geobleu'] > 0:
                print(f"  GEO-BLEU: {final_stats['geobleu']:.4f} (è®ºæ–‡ç›®æ ‡: 0.3440)")
                print(f"  DTW: {final_stats['dtw']:.4f} (è®ºæ–‡ç›®æ ‡: 29.9633)")
                
                # ä¸è®ºæ–‡å¯¹æ¯”
                geobleu_ratio = final_stats['geobleu'] / 0.3440
                dtw_ratio = final_stats['dtw'] / 29.9633 if final_stats['dtw'] != float('inf') else float('inf')
                
                print(f"  ğŸ“ˆ GEO-BLEUè¾¾åˆ°è®ºæ–‡çš„ {geobleu_ratio:.1%}")
                if dtw_ratio != float('inf'):
                    if dtw_ratio < 1:
                        print(f"  ğŸ“ˆ DTWæ¯”è®ºæ–‡å¥½ {(1-dtw_ratio)*100:.1f}%")
                    else:
                        print(f"  ğŸ“‰ DTWæ¯”è®ºæ–‡å·® {(dtw_ratio-1)*100:.1f}%")
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return self.train_history
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        model_to_save = self.model._orig_mod if hasattr(self.model, '_orig_mod') else self.model
        
        torch.save({
            'model_state_dict': model_to_save.state_dict(),
            'config': self.config,
            'train_history': self.train_history,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None
        }, path)

# =====================================================
# GPUä¼˜åŒ–é…ç½®
# =====================================================

def create_optimized_config(location_vocab_size):
    """åˆ›å»ºGPUä¼˜åŒ–çš„æ¨¡å‹é…ç½®"""
    config = {
        'max_days': 75,
        'max_times': 48,
        'max_locations': location_vocab_size,
        'max_timedelta': 101,
        'max_seq_length': 512,
        'embed_size': 256,
        'num_layers': 6,
        'num_heads': 16,
        'batch_size': 32,
        'num_epochs': 50,
        'alpha_days': 15,
        'beta_penalty': 0.9,
        'hidden_size': 1024,
        'dropout': 0.1,
        'learning_rate': 1e-4,
        'weight_decay': 0.01,
    }
    return config

def prepare_optimized_training_data(original_file, output_train_file, force_regenerate=False):
    """å‡†å¤‡GPUä¼˜åŒ–çš„è®­ç»ƒæ•°æ®"""
    if os.path.exists(output_train_file) and not force_regenerate:
        print(f"âœ… è®­ç»ƒæ•°æ®æ–‡ä»¶å·²å­˜åœ¨: {output_train_file}")
        return output_train_file
    
    print("å‡†å¤‡GPUä¼˜åŒ–è®­ç»ƒæ•°æ®...")
    df = pd.read_csv(original_file)
    
    training_data = df[
        (df['uid'] < 80000) & 
        (df['x'] != 999) & 
        (df['y'] != 999)
    ].copy()
    
    print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {training_data.shape}")
    training_data.to_csv(output_train_file, index=False)
    return output_train_file

def train_optimized_lpbert(force_regenerate_data=False, force_regenerate_sequences=False):
    """è®­ç»ƒGPUä¼˜åŒ–çš„LP-BERTæ¨¡å‹ - åŒ…å«è®ºæ–‡è¯„ä¼°"""
    print("="*60)
    print("GPUä¼˜åŒ–LP-BERTæ¨¡å‹è®­ç»ƒ (é’ˆå¯¹A100 + è®ºæ–‡è¯„ä¼°)")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆéå¸¸æ…¢ï¼‰")
    else:
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name()}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # 1. å‡†å¤‡æ•°æ®
    train_file = prepare_optimized_training_data(
        "yjmob100k-dataset1.csv", 
        "optimized_training_data.csv",
        force_regenerate_data
    )
    
    # 2. åˆ›å»ºæ•°æ®é›†
    print("\nåˆ›å»ºGPUä¼˜åŒ–è®­ç»ƒæ•°æ®é›†...")
    train_dataset = OptimizedHuMobTrainingDataset(
        train_file,
        seq_length=512,
        force_regenerate_sequences=force_regenerate_sequences
    )
    
    with open('optimized_location_vocab.pkl', 'wb') as f:
        pickle.dump(train_dataset.location_vocab, f)
    
    # 3. åˆ›å»ºé…ç½®å’Œæ•°æ®åŠ è½½å™¨
    config = create_optimized_config(len(train_dataset.location_vocab))
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        collate_fn=optimized_collate_fn,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    print(f"GPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨:")
    print(f"  æ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"  åºåˆ—é•¿åº¦: {config['max_seq_length']}")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºGPUä¼˜åŒ–LP-BERTæ¨¡å‹...")
    model = OptimizedLPBERTModel(config)
    param_count = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
    
    # 5. éªŒè¯æ•°æ®åŠ è½½å™¨
    print("\néªŒè¯GPUä¼˜åŒ–æ•°æ®åŠ è½½å™¨...")
    try:
        sample_batch = next(iter(train_loader))
        print("âœ… æ•°æ®åŠ è½½å™¨éªŒè¯æˆåŠŸ")
        print(f"  æ‰¹æ¬¡å½¢çŠ¶: {sample_batch['days'].shape}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨éªŒè¯å¤±è´¥: {e}")
        return False
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹GPUä¼˜åŒ–è®­ç»ƒï¼ˆåŒ…å«è®ºæ–‡æ ‡å‡†è¯„ä¼°ï¼‰...")
    trainer = OptimizedLPBERTTrainer(model, config)
    
    try:
        history = trainer.train(train_loader, config['num_epochs'], train_dataset.location_vocab)
        
        trainer.save_model('optimized_lpbert_final.pth')
        print(f"âœ… GPUä¼˜åŒ–æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        if history:
            total_time = sum([h['epoch_time'] for h in history])
            final_stats = history[-1]
            
            print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
            print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ")
            print(f"  æœ€ç»ˆæŸå¤±: {final_stats['train_loss']:.4f}")
            print(f"  æœ€ç»ˆç®€å•å‡†ç¡®ç‡: {final_stats['train_acc']:.2f}%")
            
            if final_stats['geobleu'] > 0:
                print(f"  æœ€ç»ˆGEO-BLEU: {final_stats['geobleu']:.4f}")
                print(f"  æœ€ç»ˆDTW: {final_stats['dtw']:.4f}")
                print(f"\nğŸ¯ ä¸è®ºæ–‡Task1å¯¹æ¯”:")
                print(f"  è®ºæ–‡GEO-BLEU: 0.3440 | ä½ çš„ç»“æœ: {final_stats['geobleu']:.4f}")
                print(f"  è®ºæ–‡DTW: 29.9633 | ä½ çš„ç»“æœ: {final_stats['dtw']:.4f}")
                
                performance = final_stats['geobleu'] / 0.3440 * 100
                print(f"  ğŸ“ˆ ä½ çš„GEO-BLEUè¾¾åˆ°è®ºæ–‡çš„{performance:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main_optimized():
    """GPUä¼˜åŒ–çš„ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GPUä¼˜åŒ–LP-BERT Human Mobility Prediction')
    parser.add_argument('--force-data', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ•°æ®')
    parser.add_argument('--force-sequences', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆåºåˆ—')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--seq-length', type=int, default=512, help='åºåˆ—é•¿åº¦')
    parser.add_argument('--epochs', type=int, default=8, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--embed-size', type=int, default=256, help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    print("GPUä¼˜åŒ–LP-BERT: Location Prediction BERT for Human Mobility")
    print("é’ˆå¯¹NVIDIA A100 80GBä¼˜åŒ–")
    print("="*80)
    
    if not os.path.exists("yjmob100k-dataset1.csv"):
        print("âŒ åŸå§‹æ•°æ®æ–‡ä»¶ yjmob100k-dataset1.csv ä¸å­˜åœ¨")
        return
    
    if torch.cuda.is_available():
        print(f"ğŸš€ GPU: {torch.cuda.get_device_name()}")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print("âœ… å¯ç”¨CUDAä¼˜åŒ–è®¾ç½®")
    
    print(f"\nğŸ”§ ä¼˜åŒ–å‚æ•°:")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  åºåˆ—é•¿åº¦: {args.seq_length}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  åµŒå…¥ç»´åº¦: {args.embed_size}")
    print(f"  å­¦ä¹ ç‡: {args.lr}")
    print(f"  æ··åˆç²¾åº¦: {'å¯ç”¨' if torch.cuda.is_available() else 'ç¦ç”¨'}")
    
    success = train_optimized_lpbert(
        force_regenerate_data=args.force_data,
        force_regenerate_sequences=args.force_sequences
    )
    
    if success:
        print("\nâœ… GPUä¼˜åŒ–LP-BERTè®­ç»ƒå®Œæˆ!")
        print("\nğŸ¯ å…³é”®æ”¹è¿›:")
        print("  âœ“ æ‰¹æ¬¡å¤§å°å¢åŠ åˆ°32 (åŸ16)")
        print("  âœ“ åºåˆ—é•¿åº¦å¢åŠ åˆ°512 (åŸ256)")
        print("  âœ“ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (FP16)")
        print("  âœ“ å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–")
        print("  âœ“ æ·»åŠ è®ºæ–‡æ ‡å‡†è¯„ä¼° (GEO-BLEU + DTW)")
        print("  âœ“ ä¿®å¤CUDAå¤šè¿›ç¨‹é—®é¢˜")
        
        print("\nğŸ’¡ è¯„ä¼°æŒ‡æ ‡è¯´æ˜:")
        print("  â€¢ ç®€å•å‡†ç¡®ç‡: è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡æ ‡")
        print("  â€¢ GEO-BLEU: è®ºæ–‡æ ‡å‡†æŒ‡æ ‡ (ç›®æ ‡0.3440)")
        print("  â€¢ DTW: è½¨è¿¹ç›¸ä¼¼æ€§æŒ‡æ ‡ (ç›®æ ‡29.9633)")
        
    else:
        print("âŒ è®­ç»ƒå¤±è´¥")
        print("\nğŸ”§ æ•…éšœæ’é™¤:")
        print("  1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ")
        print("  2. é™ä½batch_size: --batch-size 16")
        print("  3. é™ä½åºåˆ—é•¿åº¦: --seq-length 256")

if __name__ == "__main__":
    main_optimized()