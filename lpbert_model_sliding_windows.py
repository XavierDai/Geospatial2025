import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
import numpy as np
import os
import pickle
import math
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False


class LPBERTEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()
        
        # åµŒå…¥å±‚
        self.day_embedding = nn.Embedding(config['max_days'], config['embed_size'])
        self.time_embedding = nn.Embedding(config['max_times'], config['embed_size'])
        self.location_embedding = nn.Embedding(config['max_locations'], config['embed_size'])
        self.timedelta_embedding = nn.Embedding(config['max_timedelta'], config['embed_size'])
        
        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Embedding(config['seq_length'], config['embed_size'])
        
        # åµŒå…¥ç¼©æ”¾å› å­
        self.embed_scale = math.sqrt(config['embed_size'])
        
        # æ ‡å‡†åŒ–å’Œdropout
        self.layer_norm = nn.LayerNorm(config['embed_size'], eps=1e-12)
        self.dropout = nn.Dropout(config['dropout'])
        
        # åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """é’ˆå¯¹å¤§è¯æ±‡è¡¨çš„åˆå§‹åŒ–ç­–ç•¥"""
        # æˆªæ–­æ­£æ€åˆ†å¸ƒ
        std = 0.02
        
        # å¯¹location embeddingä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–
        nn.init.trunc_normal_(self.location_embedding.weight, mean=0, std=std/2, a=-2*std, b=2*std)
        
        # å…¶ä»–embeddingæ­£å¸¸åˆå§‹åŒ–
        for emb in [self.day_embedding, self.time_embedding, self.timedelta_embedding, self.position_embedding]:
            nn.init.trunc_normal_(emb.weight, mean=0, std=std, a=-2*std, b=2*std)
        
        # ç‰¹æ®Štokenåˆå§‹åŒ–
        with torch.no_grad():
            self.location_embedding.weight[0].fill_(0)  # <PAD>
            self.location_embedding.weight[1].normal_(0, std)  # <MASK>
            if 2 < len(self.location_embedding.weight):
                self.location_embedding.weight[2].normal_(0, std)  # <UNK>
    
    def forward(self, day_ids, time_ids, location_ids, timedelta_ids):
        batch_size, seq_len = day_ids.shape
        
        # ä½ç½®ç´¢å¼•
        position_ids = torch.arange(seq_len, dtype=torch.long, device=day_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # åµŒå…¥å±‚ï¼ˆlocation embeddingä½¿ç”¨ç¼©æ”¾ï¼‰
        location_embeds = self.location_embedding(location_ids) * self.embed_scale
        
        embeddings = (
            self.day_embedding(day_ids) + 
            self.time_embedding(time_ids) + 
            location_embeds + 
            self.timedelta_embedding(timedelta_ids) +
            self.position_embedding(position_ids)
        )
        
        embeddings = self.layer_norm(embeddings)
        return self.dropout(embeddings)

class LPBERTModel(nn.Module):
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        # åµŒå…¥å±‚
        self.embedding = LPBERTEmbedding(config)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config['embed_size'],
            nhead=config['num_heads'],
            dim_feedforward=config['hidden_size'],
            dropout=config['dropout'],
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-LN
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=config['num_layers']
        )
        
        # è¾“å‡ºå±‚ï¼ˆä¸¤é˜¶æ®µæŠ•å½±ï¼‰
        self.dense = nn.Linear(config['embed_size'], config['embed_size'])
        self.activation = nn.GELU()
        self.layer_norm = nn.LayerNorm(config['embed_size'], eps=1e-12)
        
        # æœ€ç»ˆé¢„æµ‹å±‚ï¼ˆä½¿ç”¨ä¸embeddingå±‚å…±äº«çš„æƒé‡ï¼‰
        self.decoder = nn.Linear(config['embed_size'], config['max_locations'], bias=False)
        
        # æƒé‡å…±äº«
        self.decoder.weight = self.embedding.location_embedding.weight
        
        # è¾“å‡ºåç½®
        self.bias = nn.Parameter(torch.zeros(config['max_locations']))
        
        # åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        # Denseå±‚åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.dense.weight)
        nn.init.zeros_(self.dense.bias)
        
        # LayerNormåˆå§‹åŒ–
        nn.init.ones_(self.layer_norm.weight)
        nn.init.zeros_(self.layer_norm.bias)
        
        # åç½®åˆå§‹åŒ–
        nn.init.zeros_(self.bias)
    
    def forward(self, day_ids, time_ids, location_ids, timedelta_ids, attention_mask=None):
        # åµŒå…¥
        embeddings = self.embedding(day_ids, time_ids, location_ids, timedelta_ids)
        
        # æ³¨æ„åŠ›æ©ç 
        if attention_mask is None:
            attention_mask = torch.ones_like(day_ids, dtype=torch.float32)
        
        # Transformerè¦æ±‚çš„padding mask
        src_key_padding_mask = (attention_mask == 0)
        
        # Transformerç¼–ç 
        encoded = self.transformer(embeddings, src_key_padding_mask=src_key_padding_mask)
        
        # è¾“å‡ºå˜æ¢
        encoded = self.dense(encoded)
        encoded = self.activation(encoded)
        encoded = self.layer_norm(encoded)
        
        # é¢„æµ‹
        logits = self.decoder(encoded) + self.bias
        
        return logits

class TrainingDataset(Dataset):
    """è®­ç»ƒæ•°æ®é›† - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, seq_length=256, mask_prob=0.15, mask_consecutive_prob=0.8):
        self.seq_length = seq_length
        self.mask_prob = mask_prob
        self.mask_consecutive_prob = mask_consecutive_prob
        
        print(f"ğŸ“ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è¯æ±‡è¡¨
        with open("cache/location_vocab.pkl", 'rb') as f:
            self.location_vocab = pickle.load(f)
        
        # åŠ è½½åºåˆ—
        with open("cache/training_sequences_256.pkl", 'rb') as f:
            cache_data = pickle.load(f)
            if isinstance(cache_data, dict) and 'sequences' in cache_data:
                sequences = cache_data['sequences']
            else:
                sequences = cache_data
        
        self.sequences = sequences
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.sequences)} æ¡åºåˆ—")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        seq = self.sequences[idx]
        
        # è·å–æ•°æ®
        days = np.array(seq['days'], dtype=np.int64)
        times = np.array(seq['times'], dtype=np.int64)
        locations = np.array(seq['locations'], dtype=np.int64)
        timedeltas = np.array(seq['timedeltas'], dtype=np.int64)
        
        # åºåˆ—é•¿åº¦å¤„ç†
        original_len = len(days)
        
        if original_len >= self.seq_length:
            # éšæœºæˆªå–
            start_idx = np.random.randint(0, original_len - self.seq_length + 1)
            end_idx = start_idx + self.seq_length
            
            days = days[start_idx:end_idx]
            times = times[start_idx:end_idx]
            locations = locations[start_idx:end_idx]
            timedeltas = timedeltas[start_idx:end_idx]
            actual_len = self.seq_length
        else:
            # å¡«å……
            pad_len = self.seq_length - original_len
            days = np.pad(days, (0, pad_len), 'constant', constant_values=0)
            times = np.pad(times, (0, pad_len), 'constant', constant_values=0) 
            locations = np.pad(locations, (0, pad_len), 'constant', constant_values=0)
            timedeltas = np.pad(timedeltas, (0, pad_len), 'constant', constant_values=1)
            actual_len = original_len
        
        # æ©ç å¤„ç†
        labels = locations.copy()
        input_locations = locations.copy()
        mask_positions = []
        
        # è¿ç»­æ©ç  vs éšæœºæ©ç 
        if np.random.random() < self.mask_consecutive_prob and actual_len > 10:
            # è¿ç»­æ©ç 
            mask_len = np.random.randint(5, min(20, actual_len // 3))
            mask_start = np.random.randint(0, actual_len - mask_len)
            
            for i in range(mask_start, mask_start + mask_len):
                if locations[i] > 0:  # ä¸æ©ç PAD
                    mask_positions.append(i)
                    input_locations[i] = 1  # <MASK>
        else:
            # éšæœºæ©ç 
            num_masks = max(1, int(actual_len * self.mask_prob))
            valid_positions = [i for i in range(actual_len) if locations[i] > 0]
            
            if valid_positions:
                mask_positions = np.random.choice(
                    valid_positions, 
                    size=min(num_masks, len(valid_positions)), 
                    replace=False
                ).tolist()
                
                for pos in mask_positions:
                    input_locations[pos] = 1  # <MASK>
        
        return {
            'days': torch.tensor(days, dtype=torch.long),
            'times': torch.tensor(times, dtype=torch.long),
            'locations': torch.tensor(input_locations, dtype=torch.long),
            'timedeltas': torch.tensor(timedeltas, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'mask_positions': mask_positions,
            'actual_length': actual_len
        }

def collate_fn(batch):
    """ä¼˜åŒ–çš„æ‰¹å¤„ç†å‡½æ•°"""
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_length = max(item['actual_length'] for item in batch)
    batch_size = len(batch)
    
    # é¢„åˆ†é…å¼ é‡
    days = torch.zeros((batch_size, max_length), dtype=torch.long)
    times = torch.zeros((batch_size, max_length), dtype=torch.long)
    locations = torch.zeros((batch_size, max_length), dtype=torch.long)
    timedeltas = torch.zeros((batch_size, max_length), dtype=torch.long)
    attention_mask = torch.zeros((batch_size, max_length), dtype=torch.float32)
    labels = torch.zeros((batch_size, max_length), dtype=torch.long)
    
    mask_positions = []
    
    for i, item in enumerate(batch):
        length = item['actual_length']
        days[i, :length] = item['days'][:length]
        times[i, :length] = item['times'][:length]
        locations[i, :length] = item['locations'][:length]
        timedeltas[i, :length] = item['timedeltas'][:length]
        attention_mask[i, :length] = 1.0
        labels[i, :length] = item['labels'][:length]
        mask_positions.append(item['mask_positions'])
    
    return {
        'days': days,
        'times': times,
        'locations': locations,
        'timedeltas': timedeltas,
        'attention_mask': attention_mask,
        'labels': labels,
        'mask_positions': mask_positions
    }

# =====================================================
# DDPè®­ç»ƒ
# =====================================================

def ddp_train_worker(rank, world_size, config):
    """DDPè®­ç»ƒè¿›ç¨‹"""
    
    # åˆå§‹åŒ–
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)
    
    if rank == 0:
        print(f"ğŸš€ å¯åŠ¨DDPè®­ç»ƒ ({world_size} GPUs)")
        print(f"é…ç½®: {config}")
    
    try:
        # æ•°æ®é›†
        dataset = TrainingDataset(
            seq_length=config['seq_length'],
            mask_prob=config['mask_prob']
        )
        
        # é‡‡æ ·å™¨
        sampler = DistributedSampler(
            dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True,
            drop_last=True
        )
        
        # æ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=config['batch_size'],
            sampler=sampler,
            collate_fn=collate_fn,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True
        )
        
        # æ¨¡å‹
        model = LPBERTModel(config).cuda()
        
        # å¹¿æ’­åˆå§‹å‚æ•°ç¡®ä¿ä¸€è‡´æ€§
        for param in model.parameters():
            dist.broadcast(param.data, src=0)
        
        model = DDP(model, device_ids=[rank])
        
        # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡ï¼‰
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['learning_rate'],
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(dataloader) * config['num_epochs']
        num_warmup_steps = int(config['warmup_ratio'] * num_training_steps)
        
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        
        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=config['label_smoothing'])
        
        # æ··åˆç²¾åº¦
        scaler = GradScaler() if AMP_AVAILABLE else None
        
        # è®­ç»ƒå¾ªç¯
        best_accuracy = 0
        
        for epoch in range(config['num_epochs']):
            sampler.set_epoch(epoch)
            model.train()
            
            total_loss = 0
            correct = 0
            total = 0
            
            # è¿›åº¦æ¡
            if rank == 0:
                pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{config["num_epochs"]}')
            else:
                pbar = dataloader
            
            for step, batch in enumerate(pbar):
                # æ•°æ®åˆ°GPU
                days = batch['days'].cuda(non_blocking=True)
                times = batch['times'].cuda(non_blocking=True)
                locations = batch['locations'].cuda(non_blocking=True)
                timedeltas = batch['timedeltas'].cuda(non_blocking=True)
                attention_mask = batch['attention_mask'].cuda(non_blocking=True)
                labels = batch['labels'].cuda(non_blocking=True)
                
                # å‰å‘ä¼ æ’­
                if AMP_AVAILABLE and scaler is not None:
                    with autocast():
                        logits = model(days, times, locations, timedeltas, attention_mask)
                        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
                    
                    # åå‘ä¼ æ’­
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    logits = model(days, times, locations, timedeltas, attention_mask)
                    loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
                    
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])
                    optimizer.step()
                
                optimizer.zero_grad()
                scheduler.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä»…æ©ç ä½ç½®ï¼‰
                with torch.no_grad():
                    predictions = torch.argmax(logits, dim=-1)
                    for i, mask_pos in enumerate(batch['mask_positions']):
                        for pos in mask_pos:
                            if pos < predictions.size(1) and pos < labels.size(1):
                                if predictions[i, pos] == labels[i, pos]:
                                    correct += 1
                                total += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                if rank == 0 and step % 20 == 0:
                    acc = correct / max(total, 1) * 100
                    current_lr = scheduler.get_last_lr()[0]
                    
                    pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'acc': f'{acc:.2f}%',
                        'lr': f'{current_lr:.2e}'
                    })
            
            # Epochç»“æŸ
            if rank == 0:
                epoch_loss = total_loss / len(dataloader)
                epoch_acc = correct / max(total, 1) * 100
                
                print(f"\nEpoch {epoch+1}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if epoch_acc > best_accuracy:
                    best_accuracy = epoch_acc
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.module.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'accuracy': epoch_acc,
                        'config': config
                    }, 'lpbert_best_optimized.pth')
                    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {epoch_acc:.2f}%)")
                
                # å®šæœŸä¿å­˜
                if (epoch + 1) % 5 == 0:
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.module.state_dict(),
                        'config': config
                    }, f'lpbert_epoch_{epoch+1}.pth')
        
        if rank == 0:
            print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    
    except Exception as e:
        print(f"âŒ GPU {rank} é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        dist.destroy_process_group()

# =====================================================
# ä¸»å‡½æ•°
# =====================================================

def create_optimized_config(vocab_size):
    """ä¼˜åŒ–çš„é…ç½®"""
    return {
        # æ•°æ®å‚æ•°
        'max_days': 75,
        'max_times': 48,
        'max_locations': vocab_size,
        'max_timedelta': 101,
        'seq_length': 256,
        
        # æ¨¡å‹å‚æ•°
        'embed_size': 256,
        'num_layers': 6,
        'num_heads': 8,
        'hidden_size': 1024,
        'dropout': 0.1,
        
        # è®­ç»ƒå‚æ•°
        'batch_size': 32,              # å¢å¤§batch size
        'num_epochs': 50,
        'learning_rate': 5e-4,         # æ›´é«˜çš„å­¦ä¹ ç‡
        'weight_decay': 0.01,
        'warmup_ratio': 0.1,           # 10% warmup
        'label_smoothing': 0.1,
        'max_grad_norm': 1.0,
        
        # æ©ç å‚æ•°
        'mask_prob': 0.15,
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LP-BERT ä¼˜åŒ–è®­ç»ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦GPU")
        return
    
    world_size = torch.cuda.device_count()
    print(f"ğŸ“Š æ£€æµ‹åˆ° {world_size} ä¸ªGPU")
    
    # åŠ è½½è¯æ±‡è¡¨è·å–å¤§å°
    with open("cache/location_vocab.pkl", 'rb') as f:
        vocab = pickle.load(f)
    
    # åˆ›å»ºé…ç½®
    config = create_optimized_config(len(vocab))
    
    print(f"\nâš™ï¸ é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    
    # å¯åŠ¨è®­ç»ƒ
    print(f"\nğŸš€ å¯åŠ¨DDPè®­ç»ƒ...")
    mp.spawn(ddp_train_worker, args=(world_size, config), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()